{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1257215,"sourceType":"datasetVersion","datasetId":723100},{"sourceId":2815545,"sourceType":"datasetVersion","datasetId":1712555},{"sourceId":3205803,"sourceType":"datasetVersion","datasetId":1918992},{"sourceId":4484183,"sourceType":"datasetVersion","datasetId":2623949},{"sourceId":6875518,"sourceType":"datasetVersion","datasetId":3950844}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:40:43.518990Z","iopub.execute_input":"2023-11-18T13:40:43.519830Z","iopub.status.idle":"2023-11-18T13:40:43.523727Z","shell.execute_reply.started":"2023-11-18T13:40:43.519795Z","shell.execute_reply":"2023-11-18T13:40:43.522796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -qqq install visualkeras\nimport visualkeras\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fake-news/FakeNewsNet.csv')\n#df = df[['title', 'real']]\ndf = df[:6000]\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:40:45.609028Z","iopub.execute_input":"2023-11-18T13:40:45.610049Z","iopub.status.idle":"2023-11-18T13:40:45.757398Z","shell.execute_reply.started":"2023-11-18T13:40:45.610012Z","shell.execute_reply":"2023-11-18T13:40:45.756422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'] = df['title'] + df['text']\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:26:31.114924Z","iopub.execute_input":"2023-11-18T12:26:31.115675Z","iopub.status.idle":"2023-11-18T12:26:31.167570Z","shell.execute_reply.started":"2023-11-18T12:26:31.115640Z","shell.execute_reply":"2023-11-18T12:26:31.166427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/financial-sentiment-analysis/data.csv')\n#df.columns = ['label', '0', '1', '2', '3', 'text']\ndf\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:08:12.566620Z","iopub.execute_input":"2023-11-11T16:08:12.567572Z","iopub.status.idle":"2023-11-11T16:08:12.607675Z","shell.execute_reply.started":"2023-11-11T16:08:12.567534Z","shell.execute_reply":"2023-11-11T16:08:12.606737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns = ['0', '1', '2', '3'], inplace = True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:32:53.466499Z","iopub.execute_input":"2023-11-18T12:32:53.466871Z","iopub.status.idle":"2023-11-18T12:32:53.478855Z","shell.execute_reply.started":"2023-11-18T12:32:53.466841Z","shell.execute_reply":"2023-11-18T12:32:53.477783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport re\nimport nltk\n#import contractions\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nexclude = string.punctuation\ndef preprocess(text):\n    text = text.lower()\n    #text = ' '.join(TextBlob(text).correct())\n    text = text.translate(str.maketrans('','', exclude))\n    text = re.sub('\\d', '', text)\n    text = re.sub(r'rt', '', text)\n    #text = ' '.join([word for word in text.split() if word not in stop_words])\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'www\\S+', '', text)\n    #text = contractions.fix(text)\n    #text = ' '.join(ps.stem(word) for word in text.split())\n    return text\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:40:58.863454Z","iopub.execute_input":"2023-11-18T13:40:58.863830Z","iopub.status.idle":"2023-11-18T13:40:59.877344Z","shell.execute_reply.started":"2023-11-18T13:40:58.863799Z","shell.execute_reply":"2023-11-18T13:40:59.876304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean'] = df['title'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:05.979547Z","iopub.execute_input":"2023-11-18T13:41:05.980394Z","iopub.status.idle":"2023-11-18T13:41:06.068494Z","shell.execute_reply.started":"2023-11-18T13:41:05.980360Z","shell.execute_reply":"2023-11-18T13:41:06.067720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:08.129360Z","iopub.execute_input":"2023-11-18T13:41:08.130118Z","iopub.status.idle":"2023-11-18T13:41:08.142229Z","shell.execute_reply.started":"2023-11-18T13:41:08.130085Z","shell.execute_reply":"2023-11-18T13:41:08.141131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Flatten, Concatenate, Attention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:12.568656Z","iopub.execute_input":"2023-11-18T13:41:12.569458Z","iopub.status.idle":"2023-11-18T13:41:17.956171Z","shell.execute_reply.started":"2023-11-18T13:41:12.569420Z","shell.execute_reply":"2023-11-18T13:41:17.955031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clustered attention\n# Tokenize the text data\nmax_words = 5000\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['clean'])\nX = tokenizer.texts_to_sequences(df['clean'])\nX = pad_sequences(X)\n\n# Encode categorical labels\nlabel_encoder = LabelEncoder()\ndf['encoded_label'] = label_encoder.fit_transform(df['real'])\nnum_classes = len(label_encoder.classes_)\n\n# Apply KMeans clustering on the tokenized text embeddings\nembedding_dim = 32  # Adjust as needed\nnum_clusters = 2  # Number of clusters\nkmeans = KMeans(n_clusters=num_clusters)\nembeddings = tf.keras.layers.Embedding(input_dim=max_words, output_dim=embedding_dim)(X)\nflat_embeddings = tf.keras.layers.Flatten()(embeddings)\nclustered_embeddings = kmeans.fit_predict(flat_embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:18.387865Z","iopub.execute_input":"2023-11-18T13:41:18.388483Z","iopub.status.idle":"2023-11-18T13:41:23.220863Z","shell.execute_reply.started":"2023-11-18T13:41:18.388448Z","shell.execute_reply":"2023-11-18T13:41:23.219912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clustered_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:24:01.907730Z","iopub.execute_input":"2023-11-17T17:24:01.908541Z","iopub.status.idle":"2023-11-17T17:24:01.915004Z","shell.execute_reply.started":"2023-11-17T17:24:01.908502Z","shell.execute_reply":"2023-11-17T17:24:01.913999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, df['encoded_label'], test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:29.161524Z","iopub.execute_input":"2023-11-18T13:41:29.162405Z","iopub.status.idle":"2023-11-18T13:41:29.169929Z","shell.execute_reply.started":"2023-11-18T13:41:29.162372Z","shell.execute_reply":"2023-11-18T13:41:29.169135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:24:56.678337Z","iopub.execute_input":"2023-11-17T17:24:56.679116Z","iopub.status.idle":"2023-11-17T17:24:56.685317Z","shell.execute_reply.started":"2023-11-17T17:24:56.679081Z","shell.execute_reply":"2023-11-17T17:24:56.684273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Create an Autoencoder Model\ndef create_autoencoder_model(input_dim, encoding_dim):\n    inputs = Input(shape=(input_dim,))\n    encoded = Dense(encoding_dim, activation='relu')(inputs)\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    autoencoder = Model(inputs=inputs, outputs=decoded)\n    autoencoder.compile(optimizer='adam', loss='mse')\n    return autoencoder\n\n# Reshape clustered_embeddings to a 2D array\nclustered_embeddings = clustered_embeddings.reshape(-1, 1)\n\n\nautoencoder = create_autoencoder_model(clustered_embeddings.shape[1], encoding_dim=32)\nautoencoder.fit(clustered_embeddings, clustered_embeddings, epochs=10, batch_size=16)\n\n\n# 3. Obtain the Encoded Representations\nencoded_embeddings = autoencoder.predict(clustered_embeddings)\nencoded_embeddings = encoded_embeddings[:y_train.shape[0]]","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:32.823211Z","iopub.execute_input":"2023-11-18T13:41:32.823602Z","iopub.status.idle":"2023-11-18T13:41:39.212699Z","shell.execute_reply.started":"2023-11-18T13:41:32.823570Z","shell.execute_reply":"2023-11-18T13:41:39.211660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:25:58.484338Z","iopub.execute_input":"2023-11-17T17:25:58.484696Z","iopub.status.idle":"2023-11-17T17:25:58.491219Z","shell.execute_reply.started":"2023-11-17T17:25:58.484666Z","shell.execute_reply":"2023-11-17T17:25:58.490208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define a simple transformer model with normal attention\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\n# Define a custom Attention layer with clustering on k and q matrices\nclass ClusteringAttention(tf.keras.layers.Layer):\n    def __init__(self, num_clusters, **kwargs):\n        super(ClusteringAttention, self).__init__(**kwargs)\n        self.num_clusters = num_clusters\n        self.q_dense = Dense(embedding_dim)\n        self.k_dense = Dense(embedding_dim)\n        self.built = False\n\n    def build(self, input_shape):\n        if not self.built:\n            super(ClusteringAttention, self).build(input_shape)\n            self.built = True\n\n    def call(self, inputs):\n        q, k, v = inputs\n\n        # Apply KMeans clustering on q and k matrices\n        k_clustered = self.k_dense(k)\n        q_clustered = self.q_dense(q)\n\n        # Perform attention mechanism\n        attention_scores = tf.matmul(q_clustered, k_clustered, transpose_b=True)\n        attention_scores = tf.nn.softmax(attention_scores)\n\n        # Apply attention scores to value matrix\n        output = tf.matmul(attention_scores, v)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[-1]\n\n# Define a simple transformer model with clustered attention\ndef create_model(attention_layer):\n    inputs = Input(shape=(X.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    if attention_layer == 'clustering':\n        # Create clustering layers\n        k_clustered_layer = Dense(num_clusters, use_bias=False)\n        q_clustered_layer = Dense(num_clusters, use_bias=False)\n\n        # Apply clustering layers to q and k\n        k_clustered = k_clustered_layer(k)\n        q_clustered = q_clustered_layer(q)\n\n        attention_layer = ClusteringAttention(num_clusters)([q_clustered, k_clustered, v])\n    else:\n        attention_layer = tf.keras.layers.Attention()([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef create_normal_attention_model():\n    inputs = Input(shape=(X.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    attention_layer = tf.keras.layers.Attention()([q, k, v])\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Define a simple transformer model with clustered+autoencoder attention\ndef create_clustered_autoencoder_attention_model():\n    inputs = Input(shape=(encoded_embeddings.shape[1],))\n    q = Dense(encoded_embeddings.shape[1])(inputs)\n    k = Dense(encoded_embeddings.shape[1])(inputs)\n    v = Dense(encoded_embeddings.shape[1])(inputs)\n\n    # Clustering Attention Layer\n    attention_layer = ClusteringAttention(num_clusters)([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train and evaluate the normal attention model\nnormal_attention_model = create_normal_attention_model()\nnormal_attention_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\ny_pred_normal = np.argmax(normal_attention_model.predict(X_test), axis=1)\naccuracy_normal = accuracy_score(y_test, y_pred_normal)\nprint(f'Test Accuracy with normal attention: {accuracy_normal}')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:44.546235Z","iopub.execute_input":"2023-11-18T13:41:44.546914Z","iopub.status.idle":"2023-11-18T13:41:48.795332Z","shell.execute_reply.started":"2023-11-18T13:41:44.546880Z","shell.execute_reply":"2023-11-18T13:41:48.794269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the clustered+autoencoder attention model\nclustered_autoencoder_attention_model = create_clustered_autoencoder_attention_model()\nclustered_autoencoder_attention_model.fit(encoded_embeddings, y_train, epochs=10, batch_size=32, validation_split=0.2)\n# Ensure y_pred_clustered_autoencoder has the same number of samples as y_test\ny_pred_clustered_autoencoder = np.argmax(clustered_autoencoder_attention_model.predict(encoded_embeddings), axis=1)\ny_pred_clustered_autoencoder = y_pred_clustered_autoencoder[:y_test.shape[0]]\n\naccuracy_clustered_autoencoder = accuracy_score(y_test, y_pred_clustered_autoencoder)\nprint(f'Test Accuracy with clustered+autoencoder attention: {accuracy_clustered_autoencoder}')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:05:51.921110Z","iopub.execute_input":"2023-11-18T12:05:51.921796Z","iopub.status.idle":"2023-11-18T12:05:56.161714Z","shell.execute_reply.started":"2023-11-18T12:05:51.921764Z","shell.execute_reply":"2023-11-18T12:05:56.160688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.callbacks import History\n\n\n\n\n# Helper function to plot accuracy and loss and save figures\ndef plot_metrics_and_save(history, metric, figure_name):\n    plt.plot(history.history[metric], label=f'Training {metric}')\n    plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric)\n    plt.legend()\n    plt.savefig(figure_name)\n    plt.show()\n\n# Helper function to calculate space and time\ndef calculate_space_and_time(model, X_test, y_test):\n    # Ensure that the number of samples in X_test matches the number of labels in y_test\n    X_test = X_test[:y_test.shape[0]]\n\n    start_time = time.time()\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    end_time = time.time()\n    accuracy = accuracy_score(y_test, y_pred)\n    execution_time = end_time - start_time\n    space_used = model.count_params()\n    \n    return accuracy, execution_time, space_used\n\n\n# Train and evaluate the normal attention model\nnormal_attention_model = create_normal_attention_model()\nhistory_normal = normal_attention_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\naccuracy_normal, time_normal, space_normal = calculate_space_and_time(normal_attention_model, X_test, y_test)\n\n# Train and evaluate the clustered+autoencoder attention model\nclustered_autoencoder_attention_model = create_clustered_autoencoder_attention_model()\nhistory_clustered_autoencoder = clustered_autoencoder_attention_model.fit(encoded_embeddings, y_train, epochs=10, batch_size=32, validation_split=0.2)\naccuracy_clustered_autoencoder, time_clustered_autoencoder, space_clustered_autoencoder = calculate_space_and_time(clustered_autoencoder_attention_model, encoded_embeddings, y_test)\n\n# Print results\nprint(f'Test Accuracy with normal attention: {accuracy_normal}')\nprint(f'Execution Time for normal attention: {time_normal} seconds')\nprint(f'Approximate Space Used by normal attention model: {space_normal} parameters\\n')\n\nprint(f'Test Accuracy with clustered+autoencoder attention: {accuracy_clustered_autoencoder}')\nprint(f'Execution Time for clustered+autoencoder attention: {time_clustered_autoencoder} seconds')\nprint(f'Approximate Space Used by clustered+autoencoder attention model: {space_clustered_autoencoder} parameters\\n')\n\n# Plot training and validation accuracy and loss\nplot_metrics_and_save(history_normal, 'accuracy', 'training_accuracy_normal.png')\nplot_metrics_and_save(history_normal, 'loss', 'training_loss_normal.png')\n\nplot_metrics_and_save(history_clustered_autoencoder, 'accuracy', 'training_accuracy_clustered_autoencoder.png')\nplot_metrics_and_save(history_clustered_autoencoder, 'loss', 'training_loss_clustered_autoencoder.png')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T13:41:53.195459Z","iopub.execute_input":"2023-11-18T13:41:53.195808Z","iopub.status.idle":"2023-11-18T13:42:04.882802Z","shell.execute_reply.started":"2023-11-18T13:41:53.195781Z","shell.execute_reply":"2023-11-18T13:42:04.881836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Helper function to plot accuracy and loss and save figures\ndef plot_metrics_and_save(history, metric, figure_name):\n    plt.plot(history.history[metric], label=f'Training {metric}')\n    plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric)\n    plt.legend()\n    plt.savefig(figure_name)\n    plt.close()\n\n# Helper function to save confusion matrix figure\ndef save_confusion_matrix(cm, labels, figure_name):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.savefig(figure_name)\n    plt.close()\n\n# Directory to save figures\nfigure_dir = 'figures'\nos.makedirs(figure_dir, exist_ok=True)\n\n# Train and evaluate models with normal attention and clustering attention\nattention_layers = ['normal', 'clustering']\nhistory_dict = {}\n\nfor attention_layer in attention_layers:\n    print(f\"Training model with {attention_layer} attention:\")\n    model = create_model(attention_layer)\n    start_time = time.time()\n    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=0)\n    end_time = time.time()\n\n    # Save training and validation accuracy figures\n    plot_metrics_and_save(history, 'accuracy', os.path.join(figure_dir, f'training_accuracy_{attention_layer}.png'))\n\n    # Save training and validation loss figures\n    plot_metrics_and_save(history, 'loss', os.path.join(figure_dir, f'training_loss_{attention_layer}.png'))\n\n    # Save confusion matrix figure\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    cm = confusion_matrix(y_test, y_pred)\n    save_confusion_matrix(cm, label_encoder.classes_, os.path.join(figure_dir, f'confusion_matrix_{attention_layer}.png'))\n\n    # Time and Space\n    execution_time = end_time - start_time\n    print(f\"Execution Time for {attention_layer} attention: {execution_time} seconds\")\n    print(f\"Approximate Space Used by Model: {model.count_params()} parameters\\n\")\n\n    # Save history for later analysis\n    history_dict[attention_layer] = history\n","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:56:55.835691Z","iopub.execute_input":"2023-11-17T17:56:55.836068Z","iopub.status.idle":"2023-11-17T17:57:01.933210Z","shell.execute_reply.started":"2023-11-17T17:56:55.836035Z","shell.execute_reply":"2023-11-17T17:57:01.932133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clustered_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:00:14.000851Z","iopub.execute_input":"2023-11-17T21:00:14.001277Z","iopub.status.idle":"2023-11-17T21:00:14.008050Z","shell.execute_reply.started":"2023-11-17T21:00:14.001241Z","shell.execute_reply":"2023-11-17T21:00:14.007032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with pca\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\n# Tokenize the text data\nmax_words = 6000\nvectorizer = TfidfVectorizer(max_features=max_words, stop_words='english')\nX_tfidf = vectorizer.fit_transform(df['clean'])\nX_dense = X_tfidf.toarray()\n\n# Encode categorical labels\nlabel_encoder = LabelEncoder()\ndf['encoded_label'] = label_encoder.fit_transform(df['real'])\nnum_classes = len(label_encoder.classes_)\n\n# Apply KMeans clustering on the tokenized text embeddings\nembedding_dim = 32  # Adjust as needed\nnum_clusters = 2  # Number of clusters\nkmeans = KMeans(n_clusters=num_clusters)\nclustered_embeddings = kmeans.fit_predict(X_dense)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_dense, df['encoded_label'], test_size=0.2, random_state=42)\n\n# Apply PCA to the clustered embeddings\npca = PCA(n_components=embedding_dim)\npca.fit(X_dense)\npca_embeddings = pca.transform(X_dense)\n\n# Define a simple transformer model with normal attention\ndef create_normal_attention_model():\n    inputs = Input(shape=(X_train.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    attention_layer = tf.keras.layers.Attention()([q, k, v])\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Define a simple transformer model with clustered+PCA attention\ndef create_clustered_pca_attention_model():\n    inputs = Input(shape=(embedding_dim,))\n    q = Dense(embedding_dim)(inputs)\n    k = Dense(embedding_dim)(inputs)\n    v = Dense(embedding_dim)(inputs)\n\n    # Clustering Attention Layer\n    attention_layer = tf.keras.layers.Attention()([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train and evaluate the normal attention model\nnormal_attention_model = create_normal_attention_model()\nhistory_normal = normal_attention_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\naccuracy_normal = accuracy_score(y_test, np.argmax(normal_attention_model.predict(X_test), axis=1))\n\n# Train and evaluate the clustered+PCA attention model\nclustered_pca_attention_model = create_clustered_pca_attention_model()\nhistory_clustered_pca = clustered_pca_attention_model.fit(pca_embeddings[:y_train.shape[0]], y_train, epochs=10, batch_size=32, validation_split=0.2)\n\naccuracy_clustered_pca = accuracy_score(y_test, np.argmax(clustered_pca_attention_model.predict(pca.transform(X_test)), axis=1))\n\n# Print results\nprint(f'Test Accuracy with normal attention: {accuracy_normal}')\nprint(f'Test Accuracy with clustered+PCA attention: {accuracy_clustered_pca}')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:20:53.901369Z","iopub.execute_input":"2023-11-17T21:20:53.901773Z","iopub.status.idle":"2023-11-17T21:27:46.882981Z","shell.execute_reply.started":"2023-11-17T21:20:53.901741Z","shell.execute_reply":"2023-11-17T21:27:46.881928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Plot training and validation accuracy\nplt.plot(history_normal.history['accuracy'], label='Normal Training Accuracy')\nplt.plot(history_normal.history['val_accuracy'], label='Normal Validation Accuracy')\nplt.plot(history_clustered_pca.history['accuracy'], label='Clustered+PCA Training Accuracy')\nplt.plot(history_clustered_pca.history['val_accuracy'], label='Clustered+PCA Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training and validation loss\nplt.plot(history_normal.history['loss'], label='Normal Training Loss')\nplt.plot(history_normal.history['val_loss'], label='Normal Validation Loss')\nplt.plot(history_clustered_pca.history['loss'], label='Clustered+PCA Training Loss')\nplt.plot(history_clustered_pca.history['val_loss'], label='Clustered+PCA Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:29:28.008475Z","iopub.execute_input":"2023-11-17T21:29:28.008890Z","iopub.status.idle":"2023-11-17T21:29:28.470086Z","shell.execute_reply.started":"2023-11-17T21:29:28.008855Z","shell.execute_reply":"2023-11-17T21:29:28.468982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:19:37.479762Z","iopub.execute_input":"2023-11-17T21:19:37.480378Z","iopub.status.idle":"2023-11-17T21:19:37.487016Z","shell.execute_reply.started":"2023-11-17T21:19:37.480339Z","shell.execute_reply":"2023-11-17T21:19:37.485944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\n# Define a custom Attention layer with clustering on k and q matrices\nclass ClusteringAttention(tf.keras.layers.Layer):\n    def __init__(self, num_clusters, **kwargs):\n        super(ClusteringAttention, self).__init__(**kwargs)\n        self.num_clusters = num_clusters\n        self.q_dense = Dense(embedding_dim)\n        self.k_dense = Dense(embedding_dim)\n        self.built = False\n\n    def build(self, input_shape):\n        if not self.built:\n            super(ClusteringAttention, self).build(input_shape)\n            self.built = True\n\n    def call(self, inputs):\n        q, k, v = inputs\n\n        # Apply KMeans clustering on q and k matrices\n        k_clustered = self.k_dense(k)\n        q_clustered = self.q_dense(q)\n\n        # Perform attention mechanism\n        attention_scores = tf.matmul(q_clustered, k_clustered, transpose_b=True)\n        attention_scores = tf.nn.softmax(attention_scores)\n\n        # Apply attention scores to value matrix\n        output = tf.matmul(attention_scores, v)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[-1]\n\n# Define a simple transformer model with clustered attention\ndef create_model(attention_layer):\n    inputs = Input(shape=(X.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    if attention_layer == 'clustering':\n        # Create clustering layers\n        k_clustered_layer = Dense(num_clusters, use_bias=False)\n        q_clustered_layer = Dense(num_clusters, use_bias=False)\n\n        # Apply clustering layers to q and k\n        k_clustered = k_clustered_layer(k)\n        q_clustered = q_clustered_layer(q)\n\n        attention_layer = ClusteringAttention(num_clusters)([q_clustered, k_clustered, v])\n    else:\n        attention_layer = tf.keras.layers.Attention()([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T10:21:46.272554Z","iopub.execute_input":"2023-11-18T10:21:46.273464Z","iopub.status.idle":"2023-11-18T10:21:46.289792Z","shell.execute_reply.started":"2023-11-18T10:21:46.273426Z","shell.execute_reply":"2023-11-18T10:21:46.288640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom Attention layer with clustering on k and q matrices\nclass ClusteringAttention(tf.keras.layers.Layer):\n    def __init__(self, num_clusters, **kwargs):\n        super(ClusteringAttention, self).__init__(**kwargs)\n        self.num_clusters = num_clusters\n        self.q_dense = Dense(embedding_dim)\n        self.k_dense = Dense(embedding_dim)\n        self.built = False\n\n    def build(self, input_shape):\n        if not self.built:\n            super(ClusteringAttention, self).build(input_shape)\n            self.built = True\n\n    def call(self, inputs):\n        q, k, v = inputs\n\n        # Apply KMeans clustering on q and k matrices\n        k_clustered = self.k_dense(k)\n        q_clustered = self.q_dense(q)\n\n        # Perform attention mechanism\n        attention_scores = tf.matmul(q_clustered, k_clustered, transpose_b=True)\n        attention_scores = tf.nn.softmax(attention_scores)\n\n        # Apply attention scores to value matrix\n        output = tf.matmul(attention_scores, v)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[-1]\n\n# Define a simple transformer model with clustered attention\ndef create_model(attention_layer):\n    inputs = Input(shape=(X.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    if attention_layer == 'clustering':\n        # Create clustering layers\n        k_clustered_layer = Dense(num_clusters, use_bias=False)\n        q_clustered_layer = Dense(num_clusters, use_bias=False)\n\n        # Apply clustering layers to q and k\n        k_clustered = k_clustered_layer(k)\n        q_clustered = q_clustered_layer(q)\n\n        attention_layer = ClusteringAttention(num_clusters)([q_clustered, k_clustered, v])\n    else:\n        attention_layer = tf.keras.layers.Attention()([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train and evaluate models with normal attention and clustered attention\nattention_layers = ['normal', 'clustering']\nfor attention_layer in attention_layers:\n    model = create_model(attention_layer)\n    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Test Accuracy with {attention_layer} attention: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T10:21:52.308920Z","iopub.execute_input":"2023-11-18T10:21:52.309787Z","iopub.status.idle":"2023-11-18T10:22:01.478608Z","shell.execute_reply.started":"2023-11-18T10:21:52.309746Z","shell.execute_reply":"2023-11-18T10:22:01.477617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:08:28.161805Z","iopub.execute_input":"2023-11-17T17:08:28.162707Z","iopub.status.idle":"2023-11-17T17:08:28.168446Z","shell.execute_reply.started":"2023-11-17T17:08:28.162662Z","shell.execute_reply":"2023-11-17T17:08:28.167537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:08:59.510414Z","iopub.execute_input":"2023-11-17T17:08:59.510799Z","iopub.status.idle":"2023-11-17T17:08:59.517839Z","shell.execute_reply.started":"2023-11-17T17:08:59.510768Z","shell.execute_reply":"2023-11-17T17:08:59.516861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_embeddings = encoded_embeddings.reshape(6000, 32)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:12:47.366538Z","iopub.execute_input":"2023-11-17T17:12:47.367581Z","iopub.status.idle":"2023-11-17T17:12:47.392233Z","shell.execute_reply.started":"2023-11-17T17:12:47.367529Z","shell.execute_reply":"2023-11-17T17:12:47.390948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom Attention layer with clustering on k and q matrices\nclass ClusteringAttention(tf.keras.layers.Layer):\n    def __init__(self, num_clusters, **kwargs):\n        super(ClusteringAttention, self).__init__(**kwargs)\n        self.num_clusters = num_clusters\n        self.q_dense = Dense(embedding_dim)\n        self.k_dense = Dense(embedding_dim)\n        self.built = False\n\n    def build(self, input_shape):\n        if not self.built:\n            super(ClusteringAttention, self).build(input_shape)\n            self.built = True\n\n    def call(self, inputs):\n        q, k, v = inputs\n\n        # Apply KMeans clustering on q and k matrices\n        k_clustered = self.k_dense(k)\n        q_clustered = self.q_dense(q)\n\n        # Perform attention mechanism\n        attention_scores = tf.matmul(q_clustered, k_clustered, transpose_b=True)\n        attention_scores = tf.nn.softmax(attention_scores)\n\n        # Apply attention scores to value matrix\n        output = tf.matmul(attention_scores, v)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[-1]\n\n# Define a simple transformer model with clustered attention\n\n# Preprocess the text data\nmax_words = 10000  # Maximum number of words to tokenize\nmax_sequence_length = 50  # Maximum sequence length for padding\nembedding_dim = 100  # Dimension of word embeddings\nnum_clusters = 100  # Number of clusters for k-means\n\n\ninputs = Input(shape=(X.shape[1],))\nembedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\nq = Dense(embedding_dim)(embedding_layer)\nk = Dense(embedding_dim)(embedding_layer)\nv = Dense(embedding_dim)(embedding_layer)\n\n# Create clustering layers\nk_clustered_layer = Dense(num_clusters, use_bias=False)\nq_clustered_layer = Dense(num_clusters, use_bias=False)\n\n# Apply clustering layers to q and k\nk_clustered = k_clustered_layer(k)\nq_clustered = q_clustered_layer(q)\n\nattention_layer = ClusteringAttention(num_clusters)([q_clustered, k_clustered, v])\noutput_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\nmodel = Model(inputs=inputs, outputs=output_layer)\nmodel.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:13:46.781757Z","iopub.execute_input":"2023-11-11T16:13:46.782111Z","iopub.status.idle":"2023-11-11T16:13:46.944168Z","shell.execute_reply.started":"2023-11-11T16:13:46.782082Z","shell.execute_reply":"2023-11-11T16:13:46.942717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:02:54.666684Z","iopub.execute_input":"2023-11-11T16:02:54.667559Z","iopub.status.idle":"2023-11-11T16:02:54.674086Z","shell.execute_reply.started":"2023-11-11T16:02:54.667500Z","shell.execute_reply":"2023-11-11T16:02:54.673156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(clustered_autoencoder_attention_model, to_file='aumodel_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T10:49:58.256202Z","iopub.execute_input":"2023-11-18T10:49:58.257148Z","iopub.status.idle":"2023-11-18T10:49:58.430608Z","shell.execute_reply.started":"2023-11-18T10:49:58.257107Z","shell.execute_reply":"2023-11-18T10:49:58.429243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:13:52.797764Z","iopub.execute_input":"2023-11-11T16:13:52.798115Z","iopub.status.idle":"2023-11-11T16:14:03.639653Z","shell.execute_reply.started":"2023-11-11T16:13:52.798085Z","shell.execute_reply":"2023-11-11T16:14:03.638670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on test data\ny_pred = np.argmax(model.predict(X_test), axis=1)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:03:14.450620Z","iopub.execute_input":"2023-11-11T16:03:14.451359Z","iopub.status.idle":"2023-11-11T16:03:14.782915Z","shell.execute_reply.started":"2023-11-11T16:03:14.451325Z","shell.execute_reply":"2023-11-11T16:03:14.781915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualkeras.layered_view(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combination of Clustered and normal attention\n# Tokenize the text data\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['clean'])\nX = tokenizer.texts_to_sequences(df['clean'])\nX = pad_sequences(X)\n\n# Encode categorical labels\nlabel_encoder = LabelEncoder()\ndf['encoded_label'] = label_encoder.fit_transform(df['real'])\nnum_classes = len(label_encoder.classes_)\n\n# Apply KMeans clustering on the tokenized text embeddings\nembedding_dim = 32  # Adjust as needed\nnum_clusters = 2  # Number of clusters\nkmeans = KMeans(n_clusters=num_clusters)\nembeddings = tf.keras.layers.Embedding(input_dim=max_words, output_dim=embedding_dim)(X)\nflat_embeddings = tf.keras.layers.Flatten()(embeddings)\nclustered_embeddings = kmeans.fit_predict(flat_embeddings)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, df['encoded_label'], test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:42:13.028144Z","iopub.execute_input":"2023-11-11T16:42:13.028798Z","iopub.status.idle":"2023-11-11T16:42:15.427495Z","shell.execute_reply.started":"2023-11-11T16:42:13.028762Z","shell.execute_reply":"2023-11-11T16:42:15.426553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a custom Attention layer with clustering on k and q matrices\nclass ClusteringAttention(tf.keras.layers.Layer):\n    def __init__(self, num_clusters, **kwargs):\n        super(ClusteringAttention, self).__init__(**kwargs)\n        self.num_clusters = num_clusters\n        self.q_dense = Dense(embedding_dim)\n        self.k_dense = Dense(embedding_dim)\n        self.built = False\n\n    def build(self, input_shape):\n        if not self.built:\n            super(ClusteringAttention, self).build(input_shape)\n            self.built = True\n\n    def call(self, inputs):\n        q, k, v = inputs\n\n        # Apply KMeans clustering on q and k matrices\n        k_clustered = self.k_dense(k)\n        q_clustered = self.q_dense(q)\n\n        # Perform attention mechanism\n        attention_scores = tf.matmul(q_clustered, k_clustered, transpose_b=True)\n        attention_scores = tf.nn.softmax(attention_scores)\n\n        # Apply attention scores to value matrix\n        output = tf.matmul(attention_scores, v)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[-1]\n\n# Define a simple transformer model with clustered attention\ndef create_model(attention_layer):\n    inputs = Input(shape=(X.shape[1],))\n    embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(inputs)\n    q = Dense(embedding_dim)(embedding_layer)\n    k = Dense(embedding_dim)(embedding_layer)\n    v = Dense(embedding_dim)(embedding_layer)\n\n    if attention_layer == 'clustering':\n        # Create clustering layers\n        k_clustered_layer = Dense(num_clusters, use_bias=False)\n        q_clustered_layer = Dense(num_clusters, use_bias=False)\n\n        # Apply clustering layers to q and k\n        k_clustered = k_clustered_layer(k)\n        q_clustered = q_clustered_layer(q)\n\n        attention_layer = ClusteringAttention(num_clusters)([q_clustered, k_clustered, v])\n    else:\n        attention_layer = tf.keras.layers.Attention()([q, k, v])\n\n    output_layer = Dense(num_classes, activation='softmax')(Flatten()(attention_layer))\n\n    model = Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:42:21.715279Z","iopub.execute_input":"2023-11-11T16:42:21.715677Z","iopub.status.idle":"2023-11-11T16:42:21.731392Z","shell.execute_reply.started":"2023-11-11T16:42:21.715640Z","shell.execute_reply":"2023-11-11T16:42:21.730292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate models with normal attention and clustered attention\nattention_layers = ['normal', 'clustering']\nfor attention_layer in attention_layers:\n    model = create_model(attention_layer)\n    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Test Accuracy with {attention_layer} attention: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:42:25.294906Z","iopub.execute_input":"2023-11-11T16:42:25.295757Z","iopub.status.idle":"2023-11-11T16:42:34.894349Z","shell.execute_reply.started":"2023-11-11T16:42:25.295723Z","shell.execute_reply":"2023-11-11T16:42:34.893354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import History\n\n# Helper function to plot accuracy and loss and save figures\ndef plot_metrics_and_save(history, metric, figure_name):\n    plt.plot(history.history[metric], label=f'Training {metric}')\n    plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric)\n    plt.legend()\n    plt.savefig(figure_name)\n    plt.close()\n\n# Helper function to save confusion matrix figure\ndef save_confusion_matrix(cm, labels, figure_name):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.savefig(figure_name)\n    plt.close()\n\n# Train and evaluate models with normal attention and clustering attention\nattention_layers = ['normal', 'clustering']\nhistory_dict = {}\n\nfor attention_layer in attention_layers:\n    print(f\"Training model with {attention_layer} attention:\")\n    model = create_model(attention_layer)\n    start_time = time.time()\n    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=0)\n    end_time = time.time()\n    \n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Test Accuracy with {attention_layer} attention: {accuracy}')\n    \n    # Save training and validation accuracy figures\n    plot_metrics_and_save(history, 'accuracy', f'training_accuracy_{attention_layer}.png')\n    \n    # Save training and validation loss figures\n    plot_metrics_and_save(history, 'loss', f'training_loss_{attention_layer}.png')\n    \n    # Save confusion matrix figure\n    cm = confusion_matrix(y_test, y_pred)\n    save_confusion_matrix(cm, label_encoder.classes_, f'confusion_matrix_{attention_layer}.png')\n\n    # Time and Space\n    execution_time = end_time - start_time\n    print(f\"Execution Time for {attention_layer} attention: {execution_time} seconds\")\n    print(f\"Approximate Space Used by Model: {model.count_params()} parameters\\n\")\n    \n    # Save history for later analysis\n    history_dict[attention_layer] = history\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T10:23:21.822191Z","iopub.execute_input":"2023-11-18T10:23:21.822578Z","iopub.status.idle":"2023-11-18T10:23:28.264056Z","shell.execute_reply.started":"2023-11-18T10:23:21.822547Z","shell.execute_reply":"2023-11-18T10:23:28.262925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Function to create and plot the model\ndef create_and_plot_model(attention_layer, figure_name):\n    model = create_model(attention_layer)\n    plot_model(model, to_file=figure_name, show_shapes=True, show_layer_names=True)\n\n# Plot normal attention model\ncreate_and_plot_model('normal', 'model_plot_normal.png')\n\n# Plot clustering attention model\ncreate_and_plot_model('clustering', 'model_plot_clustering.png')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T16:21:45.113479Z","iopub.execute_input":"2023-11-11T16:21:45.114214Z","iopub.status.idle":"2023-11-11T16:21:46.723053Z","shell.execute_reply.started":"2023-11-11T16:21:45.114175Z","shell.execute_reply":"2023-11-11T16:21:46.721689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom sklearn.cluster import KMeans\n\n\n# Preprocess the text data\nmax_words = 10000  # Maximum number of words to tokenize\nmax_sequence_length = 100  # Maximum sequence length for padding\nembedding_dim = 100  # Dimension of word embeddings\nnum_clusters = 100  # Number of clusters for k-means\n\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['clean'])\nX = tokenizer.texts_to_sequences(df['clean'])\nX = pad_sequences(X, maxlen=max_sequence_length)\n\n# Convert labels to numerical values (assuming 'pos' is 1 and 'neg' is 0)\nlabels = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build and train your LSTM model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\nmodel.add(LSTM(units=128))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the new model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test accuracy: {accuracy * 100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract embeddings from the penultimate layer\npenultimate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\nembeddings = penultimate_layer_model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom tensorflow.keras.models import Model\n#import plotly.express as px","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply t-SNE to reduce the dimensions of the embeddings to 2D\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create a scatter plot for visualization\nplt.figure(figsize=(10, 8))\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='coolwarm', s=10)  # 'labels' represent the true labels of your data points\nplt.colorbar()  # Add a colorbar to show the mapping of colors to labels\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.title('t-SNE Visualization of Penultimate Layer Embeddings')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}